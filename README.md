Download link for the llama-2 model (Note: You will need access to huggingface to download this model):

https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q8_0.gguf

Once the model is downloaded place it in the project's models folder.

Just about any gguf model will work with this demo, as long as it is compatible with your system.

Ensure you have the llama-cpp-python library installed using pip install

Run the demo from the command line using:
	python LLM_Interface.py